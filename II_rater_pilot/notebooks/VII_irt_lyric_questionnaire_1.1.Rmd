---
title: "lyric_questionnaire"
author: "andrew demetriou"
date: "11/29/2021"
---

Number of Rater Estimation:

An item response theory theory approach to analyzing the reliability of the lyric questionnaire. 

```{r setup, include=FALSE}
library('data.table') # data manipulation
library('dplyr')      # data manipulation
library('here')       # file logistics

library('mirt')       # item response theory

library("ggplot2")    # visualization
```

```{r}
# path with the actual data
data_file_path <- here("II_rater_pilot",  "response_data")

# create a list with all of the files
data_files <- list.files(data_file_path)

# read in qualtrics data file
responses_dt <- fread(here(data_file_path, "annotation_number_estimation _2.2_November 28, 2021_07.51.csv"))

# remove junk on first two rows
responses_dt <- responses_dt[3:.N]

# get question wording:
questions <- colnames(fread(here(data_file_path, data_files[1]), skip=1))

# working data table
working_dt <- responses_dt[`Participant Consent`=='agree', ]

rm(data_file_path, data_files, responses_dt)
```


```{r}
#names of lyric preferences columns for subsetting
lyric_column_names <- c(
  "Lyric_preferences_1","Lyric_preferences_2","Lyric_preferences_3",
  "Lyric_preferences_4","Lyric_preferences_5","Lyric_preferences_6",
  "Lyric_preferences_7", "Lyric_preferences_8", "Lyric_preferences_9", "Lyric_percentage_1")

#subset data table
lyrics_preferences_dt <- working_dt[, lyric_column_names, with=FALSE]

#converts each possible response into numbers for a given column
to_numbers <- function(x) {
  if(x ==  "Strongly disagree"){x = as.numeric(1)
  } else if (x== "Somewhat disagree"){x = as.numeric(2)
  } else if (x== "Neither agree nor disagree"){x = as.numeric(3)
  } else if (x=="Somewhat agree"){x = as.numeric(4)
  } else if (x=="Strongly agree"){x = as.numeric(5)
  } else {x = NA}
}

#execute recode function on relevant rows
lyrics_preferences_dt <- lyrics_preferences_dt[, .(
  L1 = lapply(Lyric_preferences_1, to_numbers), 
  L2 = lapply(Lyric_preferences_2, to_numbers), 
  L3 = lapply(Lyric_preferences_3, to_numbers),
  L4 = lapply(Lyric_preferences_4, to_numbers), 
  L5 = lapply(Lyric_preferences_5, to_numbers), 
  L6 = lapply(Lyric_preferences_6, to_numbers), 
  L7 = lapply(Lyric_preferences_7, to_numbers),
  L8 = lapply(Lyric_preferences_8, to_numbers),
  L9 = lapply(Lyric_preferences_9, to_numbers),
  L10 = Lyric_percentage_1)][, lapply(.SD, as.numeric)]

#put L10 onto the same scale
lyrics_preferences_dt$L10 <- lyrics_preferences_dt$L10 / 20

#reverse codes question L2
lyrics_preferences_dt$L2 <- 6-lyrics_preferences_dt$L2

rm(working_dt, lyric_column_names, to_numbers)
```

```{r}
#extract actual wording of questions
lyrics_questions <- questions[20:29]

#deletes repeated text
lyrics_questions <- gsub("Please indicate how much you agree with the following statements about your music preferences: - ", "", lyrics_questions)

#print questions
lyrics_questions

all_lyrics_questions <- lapply(lyrics_questions, function(x) paste0(x, "\n")) 
all_lyrics_questions <- paste(all_lyrics_questions, collapse = " ")
```

As per https://bookdown.org/bean_jerry/using_r_for_social_work_research/item-response-theory.html:

RMSEA <= .06 and SRMSR <= .08 suggest good fit. 

This model overall has poor fit

```{r}
#estimate Graded Response model (GRM)
mod_1 <- (mirt(lyrics_preferences_dt, 1, verbose = FALSE, itemtype = 'graded', SE = TRUE))

#model fit for response models with ordinal data
M2(mod_1, type = "C2", calcNULL = FALSE)


#estimate Generalized Partial Credit Model
mod_2 <- (mirt(lyrics_preferences_dt, 1, verbose = FALSE, itemtype = 'gpcm', SE = TRUE))

#model fit
M2(mod_2, type = "C2", calcNULL = FALSE)

mod <- mod_1
rm(mod_1, mod_2)
```

"The mirt implementation of S-X2 computes an RMSEA value which can be used to assess degree of item fit. Values less than .06 are considered evidence of adequate fit."

It appears the model fits all the items well. 

```{r}
itemfit(mod)
```

a is the slope: "A slope parameter is a measure of how well an item differentiates respondents with different levels of the latent trait."

"Three location parameters (b-parameters) also are listed for each item. Location parameters are interpreted as the value of theta that corresponds to a .5 probability of responding at or above that location on an item. There are m-1 location parameters where m refers to the number of response categories on the response scale."

```{r}
IRT_params <- coef(mod, IRTpars = TRUE, simplify = TRUE)
IRT_params$items
```
"Factor loadings can be interpreted as a strength of the relationship between an item and the latent variable (F1). The Communalities (h2) are squared factor loadings and are interpreted as the variance accounted for in an item by the latent trait. All of the items had a substantive relationship (loadings > .50) with the latent trait."

```{r}
summary(mod)
```
Category characteristic curves:
Item characteristic curves:

"...the probabilities of responding to specific categories in an item’s response scale...are graphically displayed in the category response curves (CRCs)"

```{r}
plot(mod, type='trace', which.item = c(1,2,3,4,5,6, 7, 8, 9, 10), facet_items=T, 
     as.table = TRUE, auto.key=list(points=F, lines=T, columns=4, space = 'top', cex = .8), 
              theta_lim = c(-3, 3), 
     main = "")
```

Item information curves:

"In polytomous models, the amount of information an item contributes depends on its slope parameter—–the larger the parameter, the more information the item provides."

"These curves show that item information is not a static quantity, rather, it is conditional on levels of theta. ...the lowest slope...the least informative item"

These curves indicate that items 3, 4, 6, and 8 contain information about the latent trait, whereas the others do not. 

```{r}
plot(mod, type='infotrace', which.item = c(1,2,3,4,5,6, 7, 8, 9, 10), facet_items=T, 
     as.table = TRUE, auto.key=list(points=F, lines=T, columns=4, space = 'top', cex = .8), 
              theta_lim = c(-3, 3), 
     main = "")
```

Scale information / test infomation and conditional standard errors:

"The solid blue line represents the scale information function. The overall scale provided the most information in the range -2.5 to + 1. The red line provides a visual reference about how estimate precision varies across theta with smaller values corresponding to better estimate precision."

This suggests that our scale was better at measuring low occurrences of the trait, rather than high occurrences. 

```{r}
plot(mod, type = 'infoSE', theta_lim = c(-3, 3), 
     main="")
```

Conditional reliability:

"This curve is mathematically related to both scale information and conditional standard errors through simple transformations. Because of this relationship, score estimates are most reliable in the -2.5 to + 1 theta range."


```{r}
plot(mod, type = 'rxx', theta_lim = c(-3, 3), 
     main="" )
```
This is a single reliability estimate:

```{r}
marginal_rxx(mod)
```

Scale characteristic curve:

"...use model parameters to generate estimates of...theta scores"
"...person parameters in IRT...factor scores in CFA..."

"...the estimates are in the theta (standard normal) metric so they are z-like scores...For example, someone with a theta score of 1.0 is one standard deviation above average and we can expect that 84% of the sample to have lower scores and 16% to have higher scores."

"A scale characteristic function provides a means of transforming estimated theta scores to expected true scores in the original scale metric."

```{r}
plot(mod, type = 'score', theta_lim = c(-3, 3), main = "")  
```

```{r}
personfit(mod)
```


